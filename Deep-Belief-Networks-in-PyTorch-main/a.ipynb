{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from DBN import DBN\n",
    "from load_dataset import MNIST\n",
    "from tqdm import trange\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST()\n",
    "train_x, train_y, test_x, test_y = mnist.load_dataset()\n",
    "\n",
    "layers = [512, 128, 64, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your image data (for demonstration purposes, let's create a dummy 3D array)\n",
    "# data = np.load('your_image_data.npy') # Your actual data loading code\n",
    "data = np.load(\"/home/hakim/sciebo2/Uni/Semester 2/BioInfo/data/data0.npy\")  # Dummy data with the same shape\n",
    "data_flattened = data.reshape(data.shape[0], -1)\n",
    "# Reshape the data: (height * width, spectrum)\n",
    "#nsamples, nx, ny = data.shape\n",
    "#reshaped_data = data.reshape((nsamples*nx, ny))\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(data_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"/home/hakim/sciebo2/Uni/Semester 2/BioInfo/data/data0.npy\")\n",
    "# Assuming 'data' is your original (291, 275, 442) array\n",
    "new = []\n",
    "for i in range(442):\n",
    "# Isolate one channel (for example, the first channel)\n",
    "    channel_data = data[:, :, i]  # Shape will be (291, 275)\n",
    "\n",
    "    # Flatten the channel data\n",
    "    channel_data_flattened = channel_data.reshape(-1)  # Flatten into a vector\n",
    "\n",
    "    # Normalize\n",
    "    scaler = StandardScaler()\n",
    "    channel_data_normalized = scaler.fit_transform(channel_data_flattened.reshape(-1, 1))\n",
    "    channel_data_normalized = channel_data_normalized.ravel()\n",
    "    new.append(channel_data_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 80025)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(new).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = torch.from_numpy(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'data' is your original (291, 275, 442) array\n",
    "n_components = 256  # Number of components for PCA\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Reshape data: Treat each point in a channel as a sample\n",
    "reshaped_data = data.reshape(-1, data.shape[2]).T  # Shape becomes (442, 291 * 275)\n",
    "\n",
    "# Apply PCA\n",
    "reduced_data = pca.fit_transform(reshaped_data)  # Shape will be (442, n_components)\n",
    "\n",
    "# Now, reduced_data can be used for training your DBN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.2994}: 100%|██████████| 100/100 [00:00<00:00, 153.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 0 to 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.1639}: 100%|██████████| 100/100 [00:00<00:00, 161.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 1 to 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.1949}: 100%|██████████| 100/100 [00:00<00:00, 161.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 2 to 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.1852}: 100%|██████████| 100/100 [00:00<00:00, 168.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 3 to 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.2178}: 100%|██████████| 100/100 [00:00<00:00, 164.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 4 to 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.2962}: 100%|██████████| 100/100 [00:00<00:00, 165.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 5 to 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.3145}: 100%|██████████| 100/100 [00:00<00:00, 168.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 6 to 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.4026}: 100%|██████████| 100/100 [00:00<00:00, 166.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 7 to 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.4356}: 100%|██████████| 100/100 [00:00<00:00, 168.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 8 to 9\n",
      "The Last layer will not be activated. The rest are activated using the Sigoid Function\n"
     ]
    }
   ],
   "source": [
    "layers = [ 256, 256,128, 128,128, 64,64,32,32]\n",
    "dbn = DBN(x_np.shape[1], layers, savefile='a.pt',gpu=True)\n",
    "dbn.train_DBN(x_np)\n",
    "\n",
    "model = dbn.initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "MAE of an all 0 reconstructor: -5.39408562261201e-09\n",
      "MAE between reconstructed and original sample: 0.11269187927246094\n"
     ]
    }
   ],
   "source": [
    "y = dbn.reconstructor(x_np)\n",
    "print('\\n\\n\\n')\n",
    "print(\"MAE of an all 0 reconstructor:\", torch.mean(x_np).item())\n",
    "print(\"MAE between reconstructed and original sample:\", torch.mean(torch.abs(y[0] - x_np)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 38, 'loss': 0.8569}:  37%|███▋      | 37/100 [00:10<00:18,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Improving the stopping training loop.\n",
      "Finished Training Layer: 0 to 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 51, 'loss': 0.4443}:  38%|███▊      | 38/100 [00:00<00:00, 138.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Improving the stopping training loop.\n",
      "Finished Training Layer: 1 to 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.3203}: 100%|██████████| 100/100 [00:00<00:00, 190.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 2 to 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.1572}: 100%|██████████| 100/100 [00:00<00:00, 191.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 3 to 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.2723}: 100%|██████████| 100/100 [00:00<00:00, 181.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 4 to 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.2059}: 100%|██████████| 100/100 [00:00<00:00, 197.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 5 to 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.313}: 100%|██████████| 100/100 [00:00<00:00, 195.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 6 to 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.2573}: 100%|██████████| 100/100 [00:00<00:00, 183.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 7 to 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 100, 'loss': 0.4324}: 100%|██████████| 100/100 [00:00<00:00, 190.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Layer: 8 to 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 31, 'loss': 0.4937}:  19%|█▉        | 19/100 [00:00<00:00, 121.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Improving the stopping training loop.\n",
      "Finished Training Layer: 9 to 10\n",
      "The Last layer will not be activated. The rest are activated using the Sigoid Function\n"
     ]
    }
   ],
   "source": [
    "layers = [512,512,256,256,128, 128,64, 64,32, 32]\n",
    "dbn = DBN(x_np.shape[1], layers, savefile='a.pt',gpu=True)\n",
    "dbn.train_DBN(x_np)\n",
    "\n",
    "model = dbn.initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "MAE of an all 0 reconstructor: 0.0\n",
      "MAE between reconstructed and original sample: 0.8223856687545776\n"
     ]
    }
   ],
   "source": [
    "y = dbn.reconstructor(x_np)\n",
    "print('\\n\\n\\n')\n",
    "print(\"MAE of an all 0 reconstructor:\", torch.mean(x_np).item())\n",
    "print(\"MAE between reconstructed and original sample:\", torch.mean(torch.abs(y[0] - x_np)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flattened = data.reshape(data.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291, 121550)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_dataset():\n",
    "\tdataset = []\n",
    "\tfor _ in range(1000):\n",
    "\t\tt = []\n",
    "\t\tfor _ in range(10):\n",
    "\t\t\tif random.random()>0.75:\n",
    "\t\t\t\tt.append(0)\n",
    "\t\t\telse:\n",
    "\t\t\t\tt.append(1)\n",
    "\t\tdataset.append(t)\n",
    "\n",
    "\tfor _ in range(1000):\n",
    "\t\tt = []\n",
    "\t\tfor _ in range(10):\n",
    "\t\t\tif random.random()>0.75:\n",
    "\t\t\t\tt.append(1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tt.append(0)\n",
    "\t\tdataset.append(t)\n",
    "\n",
    "\tdataset = np.array(dataset, dtype=np.float32)\n",
    "\tnp.random.shuffle(dataset)\n",
    "\tdataset = torch.from_numpy(dataset)\n",
    "\treturn dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 24, 'loss': 0.4868}:  20%|██        | 20/100 [00:00<00:02, 32.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Improving the stopping training loop.\n",
      "Finished Training Layer: 0 to 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 75, 'loss': 0.5524}:  72%|███████▏  | 72/100 [00:01<00:00, 38.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Improving the stopping training loop.\n",
      "Finished Training Layer: 1 to 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'epoch': 25, 'loss': 0.5602}:  22%|██▏       | 22/100 [00:00<00:02, 34.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Improving the stopping training loop.\n",
      "Finished Training Layer: 2 to 3\n",
      "The Last layer will not be activated. The rest are activated using the Sigoid Function\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MAE of an all 0 reconstructor: 0.5055999755859375\n",
      "MAE between reconstructed and original sample: 0.5000199675559998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = trial_dataset()\n",
    "\n",
    "layers = [7, 5, 2]\n",
    "\n",
    "dbn = DBN(10, layers)\n",
    "dbn.train_DBN(dataset)\n",
    "\n",
    "model = dbn.initialize_model()\n",
    "\n",
    "y = dbn.reconstructor(dataset)\n",
    "print('\\n\\n\\n')\n",
    "print(\"MAE of an all 0 reconstructor:\", torch.mean(dataset).item())\n",
    "print(\"MAE between reconstructed and original sample:\", torch.mean(torch.abs(y[0] - dataset)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "\n",
    "class RBM(nn.Module):\n",
    "    def __init__(self, n_visible, n_hidden):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.01)\n",
    "        self.h_bias = nn.Parameter(torch.zeros(n_hidden))\n",
    "        self.v_bias = nn.Parameter(torch.zeros(n_visible))\n",
    "\n",
    "    def sample_from_p(self, p):\n",
    "        return torch.bernoulli(p)\n",
    "\n",
    "    def v_to_h(self, v):\n",
    "        p_h_given_v = torch.sigmoid(torch.matmul(v, self.W.t()) + self.h_bias)\n",
    "        sample_h_given_v = self.sample_from_p(p_h_given_v)\n",
    "        return p_h_given_v, sample_h_given_v\n",
    "\n",
    "    def h_to_v(self, h):\n",
    "        p_v_given_h = torch.sigmoid(torch.matmul(h, self.W) + self.v_bias)\n",
    "        sample_v_given_h = self.sample_from_p(p_v_given_h)\n",
    "        return p_v_given_h, sample_v_given_h\n",
    "\n",
    "    def forward(self, v):\n",
    "        pre_h, h = self.v_to_h(v)\n",
    "        pre_v, v = self.h_to_v(h)\n",
    "        return pre_v\n",
    "\n",
    "    def contrastive_divergence(self, v0, k=1):\n",
    "        h0, _ = self.v_to_h(v0)\n",
    "        vk = v0\n",
    "\n",
    "        for _ in range(k):\n",
    "            _, hk = self.v_to_h(vk)\n",
    "            _, vk = self.h_to_v(hk)\n",
    "            vk = vk.detach()\n",
    "\n",
    "        phk, _ = self.v_to_h(vk)\n",
    "        positive_grad = torch.matmul(h0.t(), v0)\n",
    "        negative_grad = torch.matmul(phk.t(), vk)\n",
    "\n",
    "        dW = positive_grad - negative_grad\n",
    "        dh_bias = torch.sum(h0 - phk, dim=0)\n",
    "        dv_bias = torch.sum(v0 - vk, dim=0)\n",
    "\n",
    "        return dW, dh_bias, dv_bias\n",
    "\n",
    "    def step(self, v0, learning_rate=0.01):\n",
    "        dW, dh_bias, dv_bias = self.contrastive_divergence(v0)\n",
    "        self.W.data += learning_rate * dW\n",
    "        self.h_bias.data += learning_rate * dh_bias\n",
    "        self.v_bias.data += learning_rate * dv_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class DBN(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(DBN, self).__init__()\n",
    "        self.rbms = nn.ModuleList([RBM(n_vis, n_hid) for n_vis, n_hid in zip(layer_sizes[:-1], layer_sizes[1:])])\n",
    "\n",
    "    def pretrain(self, data, num_epochs, batch_size, learning_rate):\n",
    "        for i in range(len(self.rbms)):\n",
    "            for epoch in range(num_epochs):\n",
    "                epoch_error = 0.0\n",
    "                for j in range(0, data.size(0), batch_size):\n",
    "                    batch = data[j:j+batch_size]\n",
    "                    self.rbms[i].step(batch, learning_rate)\n",
    "                    # Calculate reconstruction error (optional)\n",
    "\n",
    "                # Print epoch error (optional)\n",
    "            \n",
    "            # Propagate data through the trained RBM for next layer's training\n",
    "            data = self._propagate_up(data, i)\n",
    "\n",
    "    def _propagate_up(self, data, layer_index):\n",
    "        with torch.no_grad():\n",
    "            for i in range(layer_index + 1):\n",
    "                _, h = self.rbms[i].v_to_h(data)\n",
    "                data = h\n",
    "        return data\n",
    "\n",
    "    def reconstruct(self, v):\n",
    "        # Reconstruct data through the network (optional)\n",
    "        pass\n",
    "    \n",
    "class RBM(nn.Module):\n",
    "    def __init__(self, n_vis, n_hid):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(n_hid, n_vis) * 0.1)\n",
    "        self.v_bias = nn.Parameter(torch.zeros(n_vis))\n",
    "        self.h_bias = nn.Parameter(torch.zeros(n_hid))\n",
    "\n",
    "    def v_to_h(self, v):\n",
    "        p_h = torch.sigmoid(F.linear(v, self.W, self.h_bias))\n",
    "        sample_h = torch.bernoulli(p_h)\n",
    "        return p_h, sample_h\n",
    "\n",
    "    def h_to_v(self, h):\n",
    "        p_v = torch.sigmoid(F.linear(h, self.W.t(), self.v_bias))\n",
    "        sample_v = torch.bernoulli(p_v)\n",
    "        return p_v, sample_v\n",
    "\n",
    "    def step(self, v, learning_rate):\n",
    "        p_h, sample_h = self.v_to_h(v)\n",
    "        p_v_, sample_v = self.h_to_v(sample_h)\n",
    "\n",
    "        positive_phase = torch.matmul(v.t(), p_h)\n",
    "        negative_phase = torch.matmul(sample_v.t(), p_h)\n",
    "\n",
    "        self.W.data += learning_rate * (positive_phase - negative_phase) / v.size(0)\n",
    "        self.v_bias.data += learning_rate * (v - sample_v).mean(0)\n",
    "        self.h_bias.data += learning_rate * (p_h - sample_h).mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(DBN, self).__init__()\n",
    "        self.rbms = nn.ModuleList([RBM(layer_sizes[i], layer_sizes[i + 1]) for i in range(len(layer_sizes) - 1)])\n",
    "        self.n_layers = len(self.rbms)\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        # Propagate up to the top\n",
    "        for rbm in self.rbms:\n",
    "            _, x = rbm.v_to_h(x)\n",
    "\n",
    "        # Propagate down to reconstruct\n",
    "        for rbm in reversed(self.rbms):\n",
    "            _, x = rbm.h_to_v(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "    def pretrain(self, data, num_epochs=10, batch_size=64, learning_rate=0.01):\n",
    "        for i in range(self.n_layers):\n",
    "            print(f\"Pretraining RBM layer {i + 1}/{self.n_layers}\")\n",
    "            for epoch in range(num_epochs):\n",
    "                epoch_error = 0.0\n",
    "                for j in range(0, data.size(0), batch_size):\n",
    "                    batch = data[j:j+batch_size]\n",
    "                    self.rbms[i].step(batch, learning_rate)\n",
    "                    v = self.rbms[i].forward(batch)\n",
    "                    epoch_error += torch.mean((batch - v) ** 2).item()\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Reconstruction Error: {epoch_error / len(data)}\")\n",
    "\n",
    "            # Propagate data through the trained RBM for next layer's training\n",
    "            data = self._propagate_up(data, i)\n",
    "\n",
    "    def _propagate_up(self, data, layer_index):\n",
    "        with torch.no_grad():\n",
    "            for i in range(layer_index + 1):\n",
    "                _, h = self.rbms[i].v_to_h(data)\n",
    "                data = h\n",
    "        return data\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for rbm in self.rbms:\n",
    "            _, x = rbm.v_to_h(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming 'data' is your original (291, 275, 442) array\n",
    "n_components = 256  # Number of components for PCA\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Reshape data: Treat each point in a channel as a sample\n",
    "reshaped_data = data.reshape(-1, data.shape[2]).T  # Shape becomes (442, 291 * 275)\n",
    "\n",
    "# Apply PCA\n",
    "reduced_data = pca.fit_transform(reshaped_data)  # Shape will be (442, n_components)\n",
    "\n",
    "# Now, reduced_data can be used for training your DBN\n",
    "reduced_data_tensor = torch.tensor(reduced_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (128) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hakim/sciebo2/Uni/Semester 2/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dbn \u001b[39m=\u001b[39m DBN([\u001b[39m256\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dbn\u001b[39m.\u001b[39;49mpretrain(reduced_data_tensor, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n",
      "\u001b[1;32m/home/hakim/sciebo2/Uni/Semester 2/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb Cell 21\u001b[0m in \u001b[0;36mDBN.pretrain\u001b[0;34m(self, data, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, data\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), batch_size):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         batch \u001b[39m=\u001b[39m data[j:j\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrbms[i]\u001b[39m.\u001b[39;49mstep(batch, learning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m# Calculate reconstruction error (optional)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Print epoch error (optional)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Propagate data through the trained RBM for next layer's training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_propagate_up(data, i)\n",
      "\u001b[1;32m/home/hakim/sciebo2/Uni/Semester 2/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb Cell 21\u001b[0m in \u001b[0;36mRBM.step\u001b[0;34m(self, v, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m positive_phase \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(v\u001b[39m.\u001b[39mt(), p_h)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m negative_phase \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(sample_v\u001b[39m.\u001b[39mt(), p_h)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m (positive_phase \u001b[39m-\u001b[39m negative_phase) \u001b[39m/\u001b[39m v\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_bias\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m (v \u001b[39m-\u001b[39m sample_v)\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X53sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh_bias\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m (p_h \u001b[39m-\u001b[39m sample_h)\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (128) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "dbn = DBN([256, 128, 128, 128, 128])\n",
    "dbn.pretrain(reduced_data_tensor, num_epochs=10, batch_size=64, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (128) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hakim/sciebo2/Uni/Semester 2/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dbn \u001b[39m=\u001b[39m DBN([\u001b[39m256\u001b[39m,\u001b[39m128\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Preprocess your dataset\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# dataset = ...\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Pretrain the DBN\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m dbn\u001b[39m.\u001b[39;49mpretrain(reduced_data_tensor, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n",
      "\u001b[1;32m/home/hakim/sciebo2/Uni/Semester 2/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb Cell 21\u001b[0m in \u001b[0;36mDBN.pretrain\u001b[0;34m(self, data, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, data\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), batch_size):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         batch \u001b[39m=\u001b[39m data[j:j\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrbms[i]\u001b[39m.\u001b[39;49mstep(batch, learning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m# Calculate reconstruction error (optional)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Print epoch error (optional)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Propagate data through the trained RBM for next layer's training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_propagate_up(data, i)\n",
      "\u001b[1;32m/home/hakim/sciebo2/Uni/Semester 2/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb Cell 21\u001b[0m in \u001b[0;36mRBM.step\u001b[0;34m(self, v, learning_rate)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m positive_phase \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(v\u001b[39m.\u001b[39mt(), p_h)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m negative_phase \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(sample_v\u001b[39m.\u001b[39mt(), p_h)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m (positive_phase \u001b[39m-\u001b[39m negative_phase) \u001b[39m/\u001b[39m v\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_bias\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m (v \u001b[39m-\u001b[39m sample_v)\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hakim/sciebo2/Uni/Semester%202/BioInfo/Deep-Belief-Networks-in-PyTorch-main/a.ipynb#X43sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh_bias\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m (p_h \u001b[39m-\u001b[39m sample_h)\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (128) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "dbn = DBN([256,128])\n",
    "\n",
    "# Preprocess your dataset\n",
    "# dataset = ...\n",
    "\n",
    "# Pretrain the DBN\n",
    "dbn.pretrain(reduced_data_tensor, num_epochs=10, batch_size=64, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reconstruction Error: 1.324916124343872\n"
     ]
    }
   ],
   "source": [
    "def test_reconstruction(dbn, test_data):\n",
    "    reconstructed_data = dbn.reconstruct(test_data)\n",
    "    reconstruction_error = ((test_data - reconstructed_data) ** 2).mean().item()\n",
    "    print(\"Average Reconstruction Error:\", reconstruction_error)\n",
    "\n",
    "# Assuming test_data is a PyTorch tensor of your test dataset\n",
    "test_reconstruction(dbn, reduced_data_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 442)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_data.reshape(256,442).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
